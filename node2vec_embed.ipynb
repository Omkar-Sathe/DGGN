{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from node2vec import Node2Vec\n",
    "import os\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load genes for making the graph\n",
    "geneidx = [] \n",
    "filea = open(\"data/gidx.txt\")\n",
    "node_gene = filea.readlines()\n",
    "for id in node_gene:\n",
    "    node = id.split()[0]\n",
    "    geneidx.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load diseases ids\n",
    "doidx = []\n",
    "filea = open(\"data/didx.txt\")\n",
    "node_disease = filea.readlines()\n",
    "for id in node_disease:\n",
    "    node_1 = id.split()[0]\n",
    "    doidx.append(node_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load lnRNA ids\n",
    "lncidx = []\n",
    "with open(\"data/lncRNA_ids.txt\", 'r') as file:\n",
    "    node_lncRNA = file.readlines()\n",
    "    for id in node_lncRNA:\n",
    "        node = id.split()[0]\n",
    "        lncidx.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load miRNA\n",
    "miidx = []\n",
    "with open('data/miRNA_ids.txt', 'r') as file:\n",
    "    node_miRNA = file.readlines()\n",
    "    for id in node_miRNA:\n",
    "        node = id.split()[0]\n",
    "        miidx.append(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "filea = open(\"data/X_train.txt\")\n",
    "x_train = filea.readlines()\n",
    "# print(x_train[0].split()[0].split(\",\")[0])\n",
    "fileb = open(\"data/y_train.txt\")\n",
    "y_train = fileb.readlines()\n",
    "\n",
    "filea = open(\"data/X_test.txt\")\n",
    "X_test = filea.readlines()\n",
    "fileb = open(\"data/y_test.txt\")\n",
    "y_test = fileb.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('G205', 'D47', '0')\n"
     ]
    }
   ],
   "source": [
    "# train set\n",
    "train = []\n",
    "for i in range(len(x_train)):\n",
    "        tup1 = (x_train[i].split()[0].split(\",\")[0],) #G123\n",
    "        tup2 = (x_train[i].split()[0].split(\",\")[1],) #D123\n",
    "        tup3 = (y_train[i].split()[0],)               #1/0\n",
    "        tup4 = tup1 + tup2 + tup3\n",
    "        # print(tup4)\n",
    "        train.append(tup4)\n",
    "print(train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test set      \n",
    "test = []\n",
    "for i in range(len(X_test)):\n",
    "        tup1 = (X_test[i].split()[0].split(\",\")[0],)\n",
    "        tup2 = (X_test[i].split()[0].split(\",\")[1],)\n",
    "        tup3 = (y_test[i].split()[0],)\n",
    "        tup4 = tup1 + tup2 + tup3\n",
    "        test.append(tup4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Graph with 1217 nodes and 217000 edges\n",
      "Before: 1217 217000\n",
      "After: 1217 5696\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d1f6adce81c4c9ebddf1b69adf92bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing transition probabilities:   0%|          | 0/1217 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "finished!\n"
     ]
    }
   ],
   "source": [
    "#embedding the 3 graphs: gd, gld, gmd\n",
    "graph_name = 'gda'\n",
    "\n",
    "gda_graph = nx.read_weighted_edgelist('data/'+graph_name+'.edgelist',delimiter=\"\\t\")  \n",
    "print(gda_graph)\n",
    "\n",
    "print(\"Before:\",gda_graph.number_of_nodes(),gda_graph.number_of_edges())\n",
    "for edge in list(gda_graph.edges):\n",
    "    if gda_graph.get_edge_data(edge[0],edge[1])['weight']<=0.0:\n",
    "        gda_graph.remove_edge(edge[0],edge[1])\n",
    "print(\"After:\",gda_graph.number_of_nodes(),gda_graph.number_of_edges())\n",
    "\n",
    "node2vec = Node2Vec(gda_graph, dimensions=64, walk_length=10, num_walks=100, workers=4) \n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)\n",
    "print(\"finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#word vectors learnt by node2vec, according to the graph\n",
    "gd_g_vec={} \n",
    "for gid in geneidx:\n",
    "    gd_g_vec[gid]=model.wv.get_vector(gid)  \n",
    "    \n",
    "gd_d_vec={}                 \n",
    "for dis in doidx:\n",
    "    gd_d_vec[dis]=model.wv.get_vector(dis)  \n",
    "\n",
    "# gd_l_vec={}\n",
    "# for lnc in lncidx:\n",
    "#     gd_l_vec[lnc] = model.wv.get_vector(lnc)\n",
    "\n",
    "# gd_m_vec = {}\n",
    "# for mi in miidx:\n",
    "#     gd_m_vec[mi] = model.wv.get_vector(mi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9112, 128) (2280, 128)\n"
     ]
    }
   ],
   "source": [
    "X_train_gd =[]          \n",
    "for gd in train:                    \n",
    "    gd_vec = np.concatenate((gd_g_vec[gd[0]],gd_d_vec[gd[1]])) \n",
    "    X_train_gd.append(gd_vec)\n",
    "        \n",
    "X_train_gd = np.array(X_train_gd)\n",
    "\n",
    "X_test_gd =[]\n",
    "for gd in test:\n",
    "    gd_vec = np.concatenate((gd_g_vec[gd[0]],gd_d_vec[gd[1]]))\n",
    "    X_test_gd.append(gd_vec)\n",
    "\n",
    "X_test_gd = np.array(X_test_gd) \n",
    "print(X_train_gd.shape, X_test_gd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save successful！！！\n"
     ]
    }
   ],
   "source": [
    "np.save('data/X_train_gd_node2vec.npy',X_train_gd) \n",
    "np.save('data/X_test_gd_node2vec.npy',X_test_gd)\n",
    "\n",
    "print(\"save successful！！！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hif1a is gene 1; use the gene name.csv for the gene ids, not the genex.csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
